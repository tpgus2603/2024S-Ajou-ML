{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Transformer Based Image Captioning",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tpgus2603/2024S-Ajou-ML/blob/main/Transformer_Based_Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'flickr-image-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F31296%2F39911%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240730%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240730T121244Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4e6f8e7b02d832ad406af2f276c8c2524d6cd89c52adfa4b59f40a6df37de439ffc10fc2ce2cd78a775f21892c65ea019d31ff03417240c68d79ba37d2c5f6495f9025373956699a76c0993ace2510c9bd8b109cbe5df15fecc88250e126adda8527df969f4c6c3d23895831757968ea5279c0c9d2d8af902f586749a9c9a2a3cdaed6e0babb3364b36205fcf5de20e34a8d35d64c1b65c524ea4afe63901a95776bcf47ed5ca45fa054eabdf479e86dd69bfca3ec4014570e2719f6c907865e812e18279b8cd265d0383a0a79d9f0daee9e286d44e9a8de2df1849e49865aa6f3a4b0eada07345da74d20c9beab42862f3b7efe1ccb9091d664abddf0bb1b89,flickr8k:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F623289%2F1111676%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240730%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240730T121244Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dad28ceacf430f47e4aa35499382bc593a16f9ec56203cc9f37cb9b01f2f54261e060b1fa026faca565b320f362b2c13044a9a0389f287963891ef587f9d2f09ac4a00112db8836df0a7bd402e1449d9de612f53677460a770fbc079e6acb719e4b84d42ae6d401f5b6ad44148cda787fca1f9226fdd836ea1767a42085242c95a2a35ddb80ecb2b0951802bcdebf4591812fa3d195ae33a8c03530a581d2cee3baf683f0cab2dbb5c69474dd49247e5b503f2314f207847551cd7cd327349fa62ac924a4a19305a4110b54de9ca75cbf626af6f8dee95685f7ced9a027ae55cf784e0203b64001ac4015e5b83967ccf611f4c8f5f4daa30457e69c1698d0a324'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "mhf4crWntDjY",
        "outputId": "99f440bf-bb26-442f-c193-e62b5d00acde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading flickr-image-dataset, 8765396518 bytes compressed\n",
            "[                                                  ] 33546240 bytes downloaded"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "cFOVkqa3tDja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('../input/flickr-image-dataset/flickr30k_images')"
      ],
      "metadata": {
        "trusted": true,
        "id": "mKgLYl8StDjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = pd.read_csv('../input/flickr-image-dataset/flickr30k_images/results.csv',delimiter='|',engine='python')\n",
        "metadata = metadata.dropna()\n",
        "is_NaN = metadata.isnull()\n",
        "row_has_NaN = is_NaN.any(axis=1)\n",
        "rows_with_NaN = metadata[row_has_NaN]\n",
        "print(rows_with_NaN)\n",
        "metadata.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "xVHDwgbmtDjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of Samples"
      ],
      "metadata": {
        "id": "JCIFCC3atDjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(metadata['image_name'].unique())"
      ],
      "metadata": {
        "trusted": true,
        "id": "RGxYOJgZtDjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the Data"
      ],
      "metadata": {
        "id": "GnV3Pr0VtDjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(name):\n",
        "    img = image.load_img(name,target_size=(32,32,3))\n",
        "    img = image.img_to_array(img)\n",
        "    #img = img/255\n",
        "    #plt.imshow(img)\n",
        "    img = np.reshape(img,(32*32*3))\n",
        "    return img"
      ],
      "metadata": {
        "trusted": true,
        "id": "BK9m_1hPtDjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_arr = []\n",
        "sentence_arr = []\n",
        "for ind in range(5000):\n",
        "    if ind % 5 != 0:\n",
        "        continue\n",
        "    image_location = (metadata.iloc[ind,:]['image_name'])\n",
        "    sentence = (metadata.iloc[ind,:][' comment'])\n",
        "\n",
        "\n",
        "    image_arr.append(load_image('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'+str(image_location)) )\n",
        "    sentence_arr.append('<SOS>'+sentence+'<EOS>')\n",
        "\n",
        "\n",
        "Images =  np.array(image_arr)"
      ],
      "metadata": {
        "trusted": true,
        "id": "GMHICX1StDje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess\n",
        "Converting to Word Embeddings\n",
        "create padding and make equal length\n",
        "Vocabulary\n",
        "The complexity of the problem is determined by the complexity of the vocabulary. A more complex vocabulary is a more complex problem. Let's look at the complexity of the dataset we'll be working with.\n",
        "\n",
        "## Tokenize (IMPLEMENTATION)\n",
        "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings. Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
        "\n",
        "We can turn each character into a number or each word into a number. These are called character and word ids, respectively. Character ids are used for character level models that generate text predictions for each character. A word level model uses word ids that generate text predictions for each word. Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
        "\n",
        "Turn each sentence into a sequence of words ids using Keras's Tokenizer function. Use this function to tokenize english_sentences and french_sentences in the cell below.\n",
        "\n",
        "Running the cell will run tokenize on sample data and show output for debugging."
      ],
      "metadata": {
        "id": "uwo_MMqztDje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(x):\n",
        "    \"\"\"\n",
        "    Tokenize x\n",
        "    :param x: List of sentences/strings to be tokenized\n",
        "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
        "    \"\"\"\n",
        "    tokenizer=Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    t=tokenizer.texts_to_sequences(x)\n",
        "    # TODO: Implement\n",
        "    return t, tokenizer\n",
        "\n",
        "# Tokenize Example output\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))"
      ],
      "metadata": {
        "trusted": true,
        "id": "QF2WLCrTtDje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding (IMPLEMENTATION)\n",
        "When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
        "\n",
        "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the end of each sequence using Keras's pad_sequences function."
      ],
      "metadata": {
        "id": "0zjPAJiytDjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(x, length=None):\n",
        "    \"\"\"\n",
        "    Pad x\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "    padding=pad_sequences(x,padding='post',maxlen=length)\n",
        "    return padding\n",
        "\n",
        "# Pad Tokenized output\n",
        "test_pad = pad(text_tokenized)\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))"
      ],
      "metadata": {
        "trusted": true,
        "id": "8D1b_njHtDjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentences):\n",
        "    text_tokenized, text_tokenizer = tokenize(sentences)\n",
        "    text_pad = pad(text_tokenized)\n",
        "    return text_pad, text_tokenizer\n",
        "\n",
        "Sentence , token_Sentence = preprocess(sentence_arr)"
      ],
      "metadata": {
        "trusted": true,
        "id": "afguepS-tDjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence vocabulary size:\", len(token_Sentence.word_index))\n",
        "print(\"Sentence Longest sentence size:\", len(Sentence[0]))"
      ],
      "metadata": {
        "trusted": true,
        "id": "onoGmxzftDjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Images.shape , Sentence.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "5bEZjCgNtDjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sentence"
      ],
      "metadata": {
        "trusted": true,
        "id": "yFqcQP4btDjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and batch data\n",
        "This tutorial uses torchtext to generate Wikitext-2 dataset. The vocab object is built based on the train dataset and is used to numericalize tokens into tensors. Starting from sequential data, the batchify() function arranges the dataset into columns, trimming off any tokens remaining after the data has been divided into batches of size batch_size. For instance, with the alphabet as the sequence (total length of 26) and a batch size of 4, we would divide the alphabet into 4 sequences of length 6:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "These columns are treated as independent by the model, which means that the dependence of G and F can not be learned, but allows more efficient batch processing."
      ],
      "metadata": {
        "id": "dNY1so28tDjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batch(src, tar , batchsize , i):\n",
        "    src, tar =  np.transpose(src[(i-1)*batchsize : (i-1)*batchsize + batchsize]) , np.transpose(tar[(i-1)*batchsize : (i-1)*batchsize + batchsize])\n",
        "    return torch.tensor(src).long(),torch.tensor(tar).long()"
      ],
      "metadata": {
        "trusted": true,
        "id": "kUg8rwLftDjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Transformer"
      ],
      "metadata": {
        "id": "2VDopGz5tDjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_len_s,\n",
        "        max_len_t,\n",
        "        device,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        self.src_position_embedding = nn.Embedding(max_len_s, embedding_size)\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
        "        self.trg_position_embedding = nn.Embedding(max_len_t, embedding_size)\n",
        "\n",
        "        self.device = device\n",
        "        self.transformer = nn.Transformer(\n",
        "            embedding_size,\n",
        "            num_heads,\n",
        "            num_encoder_layers,\n",
        "            num_decoder_layers,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "\n",
        "        # (N, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_seq_length, N = src.shape\n",
        "        trg_seq_length, N = trg.shape\n",
        "\n",
        "        src_positions = (\n",
        "            torch.arange(0, src_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(src_seq_length, N)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        trg_positions = (\n",
        "            torch.arange(0, trg_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(trg_seq_length, N)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        embed_src = self.dropout(\n",
        "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
        "        )\n",
        "        embed_trg = self.dropout(\n",
        "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
        "        )\n",
        "\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n",
        "        out = self.transformer(\n",
        "            embed_src,\n",
        "            embed_trg,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_mask=trg_mask,\n",
        "        )\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "metadata": {
        "trusted": true,
        "id": "6UNfDPYMtDjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "trusted": true,
        "id": "OVG_M3bVtDjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "src_vocab_size = 256\n",
        "trg_vocab_size = len(token_Sentence.word_index)\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.10\n",
        "max_len_s = Images.shape[1]\n",
        "max_len_t = len(Sentence[0])\n",
        "forward_expansion = 4\n",
        "src_pad_idx = 0\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-qAfMZ1DtDjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "num_epochs = 10000\n",
        "learning_rate = 3e-4\n",
        "batch_size = 1"
      ],
      "metadata": {
        "trusted": true,
        "id": "ggOc-DeNtDjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(\n",
        "    embedding_size,\n",
        "    src_vocab_size,\n",
        "    trg_vocab_size,\n",
        "    src_pad_idx,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    forward_expansion,\n",
        "    dropout,\n",
        "    max_len_s,\n",
        "    max_len_t,\n",
        "    device,\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, factor=0.1, patience=10, verbose=True\n",
        ")\n",
        "\n",
        "pad_idx = 0\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).cuda()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "sZ0m0Tm9tDjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    for i in range(1, 999):\n",
        "        src,tar = create_batch(Images,Sentence, batch_size , i)\n",
        "        src = src.to(device)\n",
        "        tar = tar.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src,tar)\n",
        "        loss = criterion(output.view(-1, output.shape[2]), tar.reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        cur_loss = loss.item()\n",
        "        total_loss += cur_loss\n",
        "        log_interval = 100\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  's/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, i, (src.shape[1]) // batch_size,\n",
        "                    elapsed  / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            start_time = time.time()\n",
        "    return total_loss"
      ],
      "metadata": {
        "trusted": true,
        "id": "2UOLcS5AtDji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "for epoch in range(1, 3):\n",
        "    epoch_start_time = time.time()\n",
        "    loss = train()\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | Training loss {:5.2f} | '\n",
        "          .format(epoch, (time.time() - epoch_start_time),\n",
        "                                     loss))"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZEHNTVJZtDji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate\n",
        "The following steps are used for evaluation:\n",
        "\n",
        "1. Encode english sentence\n",
        "2. Add tokens of start and end\n",
        "3. Decoder input is start token SOS\n",
        "4. Get the padded version of enoded sentences\n",
        "5. create mask\n",
        "Till get eos token calculate or create sentence"
      ],
      "metadata": {
        "id": "cF6gIoHStDji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image(name):\n",
        "    img = image.load_img(name,target_size=(512,512,3))\n",
        "    img = image.img_to_array(img)\n",
        "    img = img/255\n",
        "    plt.imshow(img)"
      ],
      "metadata": {
        "trusted": true,
        "id": "cXp_v_EStDji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(index):\n",
        "    image_location, sent = metadata.iloc[index,0],metadata.iloc[index,2]\n",
        "    image_arr = []\n",
        "    img = load_image('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'+str(image_location))\n",
        "    image_arr.append(img)\n",
        "    img_arr = np.array(image_arr)\n",
        "    sentence = []\n",
        "    sentence.append(sent)\n",
        "    sentence[0] = '<SOS> '+sentence[0]+'<EOS>'\n",
        "    sentence = pad(token_Sentence.texts_to_sequences(sentence) , length = max_len_t)\n",
        "    src , tar = create_batch(img_arr,sentence, 1,1)\n",
        "    src = src.to(device)\n",
        "    tar = tar.to(device)\n",
        "    model.eval()\n",
        "    output =  model(src,tar)\n",
        "    loss = criterion(output.view(-1, output.shape[2]), tar.reshape(-1))\n",
        "    sentence_formed = ''\n",
        "    val, ind = torch.max(output.view(-1, output.shape[2]), 1)\n",
        "    for word in ind:\n",
        "        #print('--->'+sentence_formed+'    '+str(word.item()))\n",
        "        if word.item() == 3: # EOS\n",
        "                break\n",
        "        for key, value in token_Sentence.word_index.items():\n",
        "            #print(value == word.item())\n",
        "            if value == word.item() and value != 2: # sos\n",
        "                sentence_formed = sentence_formed + key +' '\n",
        "                break\n",
        "    display_image('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'+str(image_location))\n",
        "    return sentence_formed , loss"
      ],
      "metadata": {
        "trusted": true,
        "id": "KtW7YyVttDji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mFKbaxUOtDji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4abQDd_3tDjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(50)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ksH5Jl9ptDjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(40)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tSU5d9SltDjj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}